{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import concatenate\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "EMBEDDING_DIM = 256\n",
        "\n",
        "lstm_layers = 2\n",
        "dropout_rate = 0.2\n",
        "learning_rate = 0.001\n",
        "\n",
        "# convert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "  all_desc = list()\n",
        "  for key in descriptions.keys():\n",
        "    [all_desc.append(d) for d in descriptions[key]]\n",
        "  return all_desc\n",
        "\n",
        "# fit a tokenizer given caption descriptions\n",
        "def create_tokenizer(descriptions):\n",
        "  lines = to_lines(descriptions)\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer\n",
        "\n",
        "\n",
        "# calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "  lines = to_lines(descriptions)\n",
        "  return max(len(d.split()) for d in lines)\n",
        "\n",
        "# create sequences of images, input sequences and output words for an image\n",
        "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "  X1, X2, y = [], [], []\n",
        "  # walk through each description for the image\n",
        "  for desc in desc_list:\n",
        "    # encode the sequence\n",
        "    seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "    # split one sequence into multiple X,y pairs\n",
        "    for i in range(1, len(seq)):\n",
        "      # split into input and output pair\n",
        "      in_seq, out_seq = seq[:i], seq[i]\n",
        "      # pad input sequence\n",
        "      in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "      # encode output sequence\n",
        "      out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "      # store\n",
        "      X1.append(photo)\n",
        "      X2.append(in_seq)\n",
        "      y.append(out_seq)\n",
        "  return np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "# data generator, intended to be used in a call to model.fit_generator()\n",
        "def data_generator(descriptions, photos, tokenizer, max_length, n_step = 1):\n",
        "  # loop for ever over images\n",
        "  while 1:\n",
        "    # loop over photo identifiers in the dataset\n",
        "    keys = list(descriptions.keys())\n",
        "    for i in range(0, len(keys), n_step):\n",
        "      Ximages, XSeq, y = list(), list(),list()\n",
        "      for j in range(i, min(len(keys), i+n_step)):\n",
        "        image_id = keys[j]\n",
        "        # retrieve the photo feature\n",
        "        photo = photos[image_id][0]\n",
        "        desc_list = descriptions[image_id]\n",
        "        in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n",
        "        for k in range(len(in_img)):\n",
        "          Ximages.append(in_img[k])\n",
        "          XSeq.append(in_seq[k])\n",
        "          y.append(out_word[k])\n",
        "      yield [[np.array(Ximages), np.array(XSeq)], np.array(y)]\n",
        "\n",
        "def categorical_crossentropy_from_logits(y_true, y_pred):\n",
        "  y_true = y_true[:, :-1, :]  # Discard the last timestep\n",
        "  y_pred = y_pred[:, :-1, :]  # Discard the last timestep\n",
        "  loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true,\n",
        "                                                 logits=y_pred)\n",
        "  return loss\n",
        "\n",
        "def categorical_accuracy_with_variable_timestep(y_true, y_pred):\n",
        "  y_true = y_true[:, :-1, :]  # Discard the last timestep\n",
        "  y_pred = y_pred[:, :-1, :]  # Discard the last timestep\n",
        "\n",
        "  # Flatten the timestep dimension\n",
        "  shape = tf.shape(y_true)\n",
        "  y_true = tf.reshape(y_true, [-1, shape[-1]])\n",
        "  y_pred = tf.reshape(y_pred, [-1, shape[-1]])\n",
        "\n",
        "  # Discard rows that are all zeros as they represent padding words.\n",
        "  is_zero_y_true = tf.equal(y_true, 0)\n",
        "  is_zero_row_y_true = tf.reduce_all(is_zero_y_true, axis=-1)\n",
        "  y_true = tf.boolean_mask(y_true, ~is_zero_row_y_true)\n",
        "  y_pred = tf.boolean_mask(y_pred, ~is_zero_row_y_true)\n",
        "\n",
        "  accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_true, axis=1),\n",
        "                                              tf.argmax(y_pred, axis=1)),\n",
        "                                    dtype=tf.float32))\n",
        "  return accuracy\n",
        "\n",
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "  # feature extractor (encoder)\n",
        "  inputs1 = Input(shape=(4096,))\n",
        "  fe1 = Dropout(0.5)(inputs1)\n",
        "  fe2 = Dense(EMBEDDING_DIM, activation='relu')(fe1)\n",
        "  fe3 = RepeatVector(max_length)(fe2)\n",
        "\n",
        "  # embedding\n",
        "  inputs2 = Input(shape=(max_length,))\n",
        "  emb2 = Embedding(vocab_size, EMBEDDING_DIM, mask_zero=True)(inputs2)\n",
        "\n",
        "  # merge inputs\n",
        "  merged = concatenate([fe3, emb2])\n",
        "  # language model (decoder)\n",
        "  lm2 = LSTM(500, return_sequences=False)(merged)\n",
        "  #lm3 = Dense(500, activation='relu')(lm2)\n",
        "  outputs = Dense(vocab_size, activation='softmax')(lm2)\n",
        "\n",
        "  # tie it together [image, seq] [word]\n",
        "  model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  print(model.summary())\n",
        "  plot_model(model, show_shapes=True, to_file='model.png')\n",
        "  return model\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}