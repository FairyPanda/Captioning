{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pickle import load\n",
        "import argparse\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "  # open the file as read only\n",
        "  file = open(filename, 'r')\n",
        "  # read all text\n",
        "  text = file.read()\n",
        "  # close the file\n",
        "  file.close()\n",
        "  return text\n",
        "\n",
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "  doc = load_doc(filename)\n",
        "  dataset = list()\n",
        "  # process line by line\n",
        "  for line in doc.split('\\n'):\n",
        "    # skip empty lines\n",
        "    if len(line) < 1:\n",
        "      continue\n",
        "    # get the image identifier\n",
        "    identifier = line.split('.')[0]\n",
        "    dataset.append(identifier)\n",
        "  return set(dataset)\n",
        "\n",
        "# split a dataset into train/test elements\n",
        "def train_test_split(dataset):\n",
        "  # order keys so the split is consistent\n",
        "  ordered = sorted(dataset)\n",
        "  # return split dataset as two new sets\n",
        "  return set(ordered[:100]), set(ordered[100:200])\n",
        "\n",
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "  # load document\n",
        "  doc = load_doc(filename)\n",
        "  descriptions = dict()\n",
        "  for line in doc.split('\\n'):\n",
        "    # split line by white space\n",
        "    tokens = line.split()\n",
        "    # split id from description\n",
        "    image_id, image_desc = tokens[0], tokens[1:]\n",
        "    # skip images not in the set\n",
        "    if image_id in dataset:\n",
        "      # create list\n",
        "      if image_id not in descriptions:\n",
        "        descriptions[image_id] = list()\n",
        "      # wrap description in tokens\n",
        "      desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "      # store\n",
        "      descriptions[image_id].append(desc)\n",
        "  return descriptions\n",
        "\n",
        "# load photo features\n",
        "def load_photo_features(filename, dataset):\n",
        "  # load all features\n",
        "  all_features = load(open(filename, 'rb'))\n",
        "  # filter features\n",
        "  features = {k: all_features[k] for k in dataset}\n",
        "  return features\n",
        "\n",
        "def prepare_dataset(data='dev'):\n",
        "\n",
        "  assert data in ['dev', 'train', 'test']\n",
        "\n",
        "  train_features = None\n",
        "  train_descriptions = None\n",
        "\n",
        "  if data == 'dev':\n",
        "    # load dev set (1K)\n",
        "    filename = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
        "    dataset = load_set(filename)\n",
        "    print('Dataset: %d' % len(dataset))\n",
        "\n",
        "    # train-test split\n",
        "    train, test = train_test_split(dataset)\n",
        "    #print('Train=%d, Test=%d' % (len(train), len(test)))\n",
        "\n",
        "    # descriptions\n",
        "    train_descriptions = load_clean_descriptions('models/descriptions.txt', train)\n",
        "    test_descriptions = load_clean_descriptions('models/descriptions.txt', test)\n",
        "    print('Descriptions: train=%d, test=%d' % (len(train_descriptions), len(test_descriptions)))\n",
        "\n",
        "    # photo features\n",
        "    train_features = load_photo_features('models/features.pkl', train)\n",
        "    test_features = load_photo_features('models/features.pkl', test)\n",
        "    print('Photos: train=%d, test=%d' % (len(train_features), len(test_features)))\n",
        "\n",
        "  elif data == 'train':\n",
        "    # load training dataset (6K)\n",
        "    filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
        "    train = load_set(filename)\n",
        "\n",
        "    filename = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
        "    test = load_set(filename)\n",
        "    print('Dataset: %d' % len(train))\n",
        "\n",
        "    # descriptions\n",
        "    train_descriptions = load_clean_descriptions('models/descriptions.txt', train)\n",
        "    test_descriptions = load_clean_descriptions('models/descriptions.txt', test)\n",
        "    print('Descriptions: train=%d, test=%d' % (len(train_descriptions), len(test_descriptions)))\n",
        "\n",
        "    # photo features\n",
        "    train_features = load_photo_features('models/features.pkl', train)\n",
        "    test_features = load_photo_features('models/features.pkl', test)\n",
        "    print('Photos: train=%d, test=%d' % (len(train_features), len(test_features)))\n",
        "\n",
        "  elif data == 'test':\n",
        "    # load test set\n",
        "    filename = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
        "    test = load_set(filename)\n",
        "    print('Dataset: %d' % len(test))\n",
        "    # descriptions\n",
        "    test_descriptions = load_clean_descriptions('models/descriptions.txt', test)\n",
        "    print('Descriptions: test=%d' % len(test_descriptions))\n",
        "    # photo features\n",
        "    test_features = load_photo_features('models/features.pkl', test)\n",
        "    print('Photos: test=%d' % len(test_features))\n",
        "\n",
        "  return (train_features, train_descriptions), (test_features, test_descriptions)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  parser = argparse.ArgumentParser(description='Generate dataset features')\n",
        "  parser.add_argument(\"-t\", \"--train\", action='store_const', const='train',\n",
        "    default = 'dev', help=\"Use large 6K training set\")\n",
        "  args = parser.parse_args()\n",
        "  prepare_dataset(args.train)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}