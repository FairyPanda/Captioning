{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import load_data as ld\n",
        "import generate_model as gen\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from pickle import dump\n",
        "\n",
        "def train_model(weight = None, epochs = 10):\n",
        "  # load dataset\n",
        "  data = ld.prepare_dataset('train')\n",
        "  train_features, train_descriptions = data[0]\n",
        "  test_features, test_descriptions = data[1]\n",
        "\n",
        "  # prepare tokenizer\n",
        "  tokenizer = gen.create_tokenizer(train_descriptions)\n",
        "  # save the tokenizer\n",
        "  dump(tokenizer, open('models/tokenizer.pkl', 'wb'))\n",
        "  # index_word dict\n",
        "  index_word = {v: k for k, v in tokenizer.word_index.items()}\n",
        "  # save dict\n",
        "  dump(index_word, open('models/index_word.pkl', 'wb'))\n",
        "\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "  print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "  # determine the maximum sequence length\n",
        "  max_length = gen.max_length(train_descriptions)\n",
        "  print('Description Length: %d' % max_length)\n",
        "\n",
        "  # generate model\n",
        "  model = gen.define_model(vocab_size, max_length)\n",
        "\n",
        "  # Check if pre-trained weights to be used\n",
        "  if weight != None:\n",
        "    model.load_weights(weight)\n",
        "\n",
        "  # define checkpoint callback\n",
        "  filepath = 'models/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1,\n",
        "                save_best_only=True, mode='min')\n",
        "\n",
        "  steps = len(train_descriptions)\n",
        "  val_steps = len(test_descriptions)\n",
        "  # create the data generator\n",
        "  train_generator = gen.data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
        "  val_generator = gen.data_generator(test_descriptions, test_features, tokenizer, max_length)\n",
        "\n",
        "  # fit model\n",
        "  model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=steps, verbose=1,\n",
        "        callbacks=[checkpoint], validation_data=val_generator, validation_steps=val_steps)\n",
        "\n",
        "  try:\n",
        "      model.save('models/wholeModel.h5', overwrite=True)\n",
        "      model.save_weights('models/weights.h5',overwrite=True)\n",
        "  except:\n",
        "      print(\"Error in saving model.\")\n",
        "  print(\"Training complete...\\n\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_model(epochs=20)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}